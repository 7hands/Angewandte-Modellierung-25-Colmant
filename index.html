---
layout: default
title: "Dokumentation: Gesichtserkennung mit Faster R-CNN und MobileNet v3"
---

<style>
    body {
        font-family: sans-serif;
        line-height: 1.6;
        max-width: 800px;
        margin: 2rem auto;
        padding: 0 1rem;
    }
    h1, h2, h3 {
        color: #333;
        margin-top: 2rem;
    }
    pre code {
        background: #f5f5f5;
        display: block;
        padding: 1rem;
        overflow-x: auto;
        border-radius: 4px;
    }
    .image-pair {
        display: flex;
        flex-wrap: wrap;
        gap: 1rem;
        margin-bottom: 2rem;
        align-items: center;
    }
    .image-pair img {
        max-width: 100%;
        height: auto;
        border: 1px solid #ddd;
        border-radius: 4px;
    }
    .image-pair .inference {
        flex: 1 1 45%;
    }
    .image-pair .original {
        flex: 1 1 45%;
    }
    .caption {
        text-align: center;
        font-size: 0.9rem;
        color: #666;
    }
    a {
        color: #0366d6;
    }
    nav#toc {
        background: #f9f9f9;
        padding: 1rem;
        border-radius: 4px;
        margin-bottom: 2rem;
    }
    nav#toc ul {
        list-style: none;
        padding-left: 0;
    }
    nav#toc li {
        margin: 0.5rem 0;
    }
    .slider {
      display: flex;
      overflow-x: auto;
      scroll-snap-type: x mandatory;
      gap: 1rem;
      margin-bottom: 2rem;
      border: 1px solid #ccc;
      padding: 1rem;
    }
    .slide {
      flex: 0 0 100%;
      scroll-snap-align: start;
    }
    .slide img {
      width: 100%;
      height: auto;
      display: block;
      border-radius: 4px;
      border: 1px solid #ddd;
    }
    .caption {
      text-align: center;
      font-size: 0.9rem;
      margin-top: 0.5rem;
      color: #666;
    }
</style>

<h1>Dokumentation: Gesichtserkennung mit Faster R-CNN und MobileNet v3</h1>
<p><em>Angewandte Modellierung – Projektarbeit</em></p>
<p><small>Stand: {{ site.time | date: "%d.%m.%Y" }}</small></p>

<nav id="toc">
  <strong>Inhalt</strong>
  <ul>
    <li><a href="#motivation">Motivation</a></li>
    <li><a href="#dataset">Dataset</a></li>
    <li><a href="#model">Modell und Aufbau</a></li>
    <li><a href="#pipeline">Pipeline</a></li>
    <li><a href="#results">Ergebnisse</a></li>
    <li><a href="#conclusion">Fazit</a></li>
    <li><a href="#sources">Quellen</a></li>
  </ul>
</nav>

<section id="motivation">
  <h2>1. Motivation</h2>
  <p>
    Vision AI interessiert mich schon seit ein paar Jahren (synthetische Frames mit NVIDIA RTX, FSD 2019 &amp; 2022). 
    Für dieses Projekt war spannend, wie gut man heute Vision AI lokal trainieren und einsetzen kann – 
    als Beispiel: Gesichtserkennung.
  </p>
</section>

<section id="dataset">
  <h2>2. Dataset</h2>
  <p>
    Bei lokalem Training gilt es, Datengröße und Trainingsdauer abzuwägen. 
    Ich habe das <strong>WIDER Face</strong> Dataset gewählt, extrahiert aus dem vollständigen WIDER Dataset.
  </p>
  <h3>2.1 WIDER Face</h3>
  <ul>
    <li>32.203 Bilder, 393.703 Gesichter (Train/Val/Test)</li>
    <li>Hohe Variabilität in Szene, Beleuchtung, Auflösung, Ausdruck</li>
  </ul>
  <h3>2.2 Annotations</h3>
  <p>Ground‑Truth in Textdateien, Einträge:</p>
  <ul>
    <li><code>x y w h</code>: obere linke Ecke + Breite/Höhe</li>
    <li>Attribute: blur, expression, illumination, invalid, occlusion, pose (0/1)</li>
  </ul>
  <p>Ungültige Einträge (invalid=1) werden gefiltert, Labels: 1=Gesicht, 0=Hintergrund.</p>
</section>

<section id="model">
  <h2>3. Modell und Aufbau</h2>
  <h3>3.1 Faster R-CNN</h3>
  <ol>
    <li>RPN (Region Proposal Network)</li>
    <li>Klassifikation &amp; Box‑Regression</li>
  </ol>
  <h3>3.2 MobileNet v3 large + FPN</h3>
  <ul>
    <li>leichter Backbone, Echtzeit‑tauglich</li>
    <li>FPN für Multi‑Scale‑Erkennung</li>
    <li>num_classes=2 (Gesicht vs. Hintergrund)</li>
  </ul>
</section>

<section id="pipeline">
  <h2>4. Pipeline</h2>
  <h3>4.1 Daten laden</h3>
  <pre><code class="language-python">
train_records = load_widerface_annotation(train_txt, train_images)
val_records   = load_widerface_annotation(val_txt, val_images)

train_ds = ObjectDetectionDataset(train_records, transforms=get_transforms(train=True))
val_ds   = ObjectDetectionDataset(val_records, transforms=get_transforms(train=False))
  </code></pre>
  <h3>4.2 Training</h3>
  <ul>
    <li>Epochen: 10</li>
    <li>Berechnung von Train- und Validation‑Loss</li>
  </ul>
  <pre><code class="language-python">
for epoch in range(num_epochs):
    # Training
    ...
    # Validation
    ...
</code></pre>
  <h3>4.3 Gewichte speichern</h3>
  <p><code>torch.save(model.state_dict(), 'checkpoints/...')</code></p>
  <h3>4.4 Inference</h3>
  <pre><code class="language-python">
def run_inference(model, image_paths, output_dir, device, threshold=0.5):
    ...
    for img_path in image_paths:
        ...
        ToPILImage()(img_boxes).save(save_path)
</code></pre>
</section>

<section id="results">
  <h2>5. Ergebnisse – Slideshow</h2>
  <div class="slider" id="slider"></div>

  <script>
    // Manuell gepflegte Liste der Bilddateien im Ordner
    const imageFilenames = [
      "common-emotions.jpg",
      "istockphoto.jpg",
      "people.jpg",
      "ocluded.png",
      "foggy-painting.jpg"
      // weitere Bilder hier ergänzen
    ];

    const slider = document.getElementById("slider");

    imageFilenames.forEach(filename => {
      const slide = document.createElement("div");
      slide.className = "slide";

      const img = document.createElement("img");
      img.src = "Vortrag_Projeckt/inference_results/" + filename;
      img.alt = filename;

      const caption = document.createElement("div");
      caption.className = "caption";
      caption.textContent = filename;

      slide.appendChild(img);
      slide.appendChild(caption);
      slider.appendChild(slide);
    });
  </script>
</section>

<section id="conclusion">
  <h2>6. Fazit</h2>
  <p>
    Lokale Gesichtserkennung mit Faster R‑CNN &amp; MobileNet v3 ist effizient realisierbar. 
    Mit mehr Daten, alternativen Backbones oder Quantisierung lassen sich Leistung und Geschwindigkeit weiter steigern.
  </p>
  <ul>
    <li>Datenerweiterung &amp; Augmentation</li>
    <li>Feintuning von ResNet/EfficientNet</li>
    <li>Inferenz‑Optimierung (TensorRT, Quantisierung)</li>
  </ul>
</section>

<section id="sources">
  <h2>7. Quellen</h2>
  <ul>
    <li><a href="https://www.kaggle.com/datasets/mksaad/wider-face-a-face-detection-benchmark/data">WIDER Face Dataset auf Kaggle</a></li>
    <li>Yang, Shuo, et al. WIDER FACE: A Face Detection Benchmark. CVPR 2016.</li>
    <li>Ren, Shaoqing, et al. Faster R‑CNN. IEEE TPAMI 2017.</li>
    <li>Howard, Andrew, et al. Searching for MobileNetV3. ICCV 2019.</li>
    <li><a href="https://pytorch.org/vision/stable/models.html">Torchvision Models</a></li>
  </ul>
</section>
